{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14167655,"sourceType":"datasetVersion","datasetId":9030784}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q transformers peft accelerate gradio\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-17T23:30:38.945267Z","iopub.execute_input":"2025-12-17T23:30:38.946016Z","iopub.status.idle":"2025-12-17T23:31:52.398463Z","shell.execute_reply.started":"2025-12-17T23:30:38.945991Z","shell.execute_reply":"2025-12-17T23:31:52.397711Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m68.6/68.6 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m444.8/444.8 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m89.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m73.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nthinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!ls /kaggle/input/mistral-model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T23:31:52.400063Z","iopub.execute_input":"2025-12-17T23:31:52.400361Z","iopub.status.idle":"2025-12-17T23:31:52.525568Z","shell.execute_reply.started":"2025-12-17T23:31:52.400334Z","shell.execute_reply":"2025-12-17T23:31:52.524888Z"}},"outputs":[{"name":"stdout","text":"adapter_config.json\t   checkpoint-417  README.md\nadapter_model.safetensors  checkpoint-834\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!ls /kaggle/input/mistral-model/mistral_lora_adapter\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T23:31:52.526515Z","iopub.execute_input":"2025-12-17T23:31:52.526779Z","iopub.status.idle":"2025-12-17T23:31:52.643680Z","shell.execute_reply.started":"2025-12-17T23:31:52.526749Z","shell.execute_reply":"2025-12-17T23:31:52.643088Z"}},"outputs":[{"name":"stdout","text":"ls: cannot access '/kaggle/input/mistral-model/mistral_lora_adapter': No such file or directory\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!ls /kaggle/input\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T23:31:52.645660Z","iopub.execute_input":"2025-12-17T23:31:52.645937Z","iopub.status.idle":"2025-12-17T23:31:52.761297Z","shell.execute_reply.started":"2025-12-17T23:31:52.645901Z","shell.execute_reply":"2025-12-17T23:31:52.760203Z"}},"outputs":[{"name":"stdout","text":"mistral-model\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import PeftModel\nimport torch\nimport os\n\nBASE_MODEL = \"mistralai/Mistral-7B-Instruct-v0.2\"\nLORA_PATH = \"/kaggle/input/mistral-model/checkpoint-834\"\nOFFLOAD_DIR = \"/kaggle/working/offload\"\n\nos.makedirs(OFFLOAD_DIR, exist_ok=True)\n\nprint(\"Loading tokenizer...\")\ntokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n\nprint(\"Loading base model with offloading...\")\nbase_model = AutoModelForCausalLM.from_pretrained(\n    BASE_MODEL,\n    device_map=\"auto\",\n    torch_dtype=torch.float16,\n    offload_folder=OFFLOAD_DIR\n)\n\nprint(\"Loading LoRA adapter...\")\nmodel = PeftModel.from_pretrained(base_model, LORA_PATH)\nmodel.eval()\n\nprint(\"âœ… Model + LoRA loaded successfully\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T23:31:52.762297Z","iopub.execute_input":"2025-12-17T23:31:52.762534Z","iopub.status.idle":"2025-12-17T23:33:37.618427Z","shell.execute_reply.started":"2025-12-17T23:31:52.762512Z","shell.execute_reply":"2025-12-17T23:33:37.617690Z"}},"outputs":[{"name":"stderr","text":"2025-12-17 23:32:05.398033: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1766014325.580125      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1766014325.634684      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"name":"stdout","text":"Loading tokenizer...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dff41c654b3346aa841fd88e0594d22c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"273072b15a414ea09f049af296e78f88"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"206de1d327e84616900979cf2165771d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc53484860c04f9e8bc00364c8062e80"}},"metadata":{}},{"name":"stdout","text":"Loading base model with offloading...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/596 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8ed505f10844414bf45e5f9b2f70191"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ceba026bddc42bb8345ea688b811f53"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47ee19a17654481a8a5a20f231883d7f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58ec5304dfd240a5b229f0e686a00da0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3950a175fd9c4d41bfb444cb67c6f29a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"795d96feb2e74567b146352dafc08d72"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"385361d05fc1451a8fbdded1067c9df3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d0a36e944cd4c6eb97b8f374688a875"}},"metadata":{}},{"name":"stdout","text":"Loading LoRA adapter...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/peft/config.py:165: UserWarning: Unexpected keyword arguments ['alora_invocation_tokens', 'arrow_config', 'ensure_weight_tying', 'peft_version', 'target_parameters'] for class LoraConfig, these are ignored. This probably means that you're loading a configuration file that was saved using a higher version of the library and additional parameters have been introduced since. It is highly recommended to upgrade the PEFT version before continuing (e.g. by running `pip install -U peft`).\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"âœ… Model + LoRA loaded successfully\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import PeftModel\nimport torch\nimport os\n\nBASE_MODEL = \"mistralai/Mistral-7B-Instruct-v0.2\"\nLORA_PATH = \"/kaggle/input/mistral-model/checkpoint-834\"\nOFFLOAD_DIR = \"/kaggle/working/offload\"\n\nos.makedirs(OFFLOAD_DIR, exist_ok=True)\n\nprint(\"Loading tokenizer...\")\ntokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n\nprint(\"Loading base model with offloading...\")\nbase_model = AutoModelForCausalLM.from_pretrained(\n    BASE_MODEL,\n    device_map=\"auto\",\n    torch_dtype=torch.float16,\n    offload_folder=OFFLOAD_DIR\n)\n\nprint(\"Loading LoRA adapter...\")\nmodel = PeftModel.from_pretrained(\n    base_model,\n    LORA_PATH,\n    is_trainable=False,\n    offload_folder=OFFLOAD_DIR   # ğŸ‘ˆ Ø§Ù„Ø³Ø·Ø± Ø§Ù„Ø­Ø§Ø³Ù…\n)\n\nmodel.eval()\n\nprint(\"âœ… Model + LoRA loaded successfully\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T23:33:37.619448Z","iopub.execute_input":"2025-12-17T23:33:37.619768Z","iopub.status.idle":"2025-12-17T23:33:57.085625Z","shell.execute_reply.started":"2025-12-17T23:33:37.619739Z","shell.execute_reply":"2025-12-17T23:33:57.084954Z"}},"outputs":[{"name":"stdout","text":"Loading tokenizer...\nLoading base model with offloading...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"957003675ca04eaa9527731cd9879bb3"}},"metadata":{}},{"name":"stdout","text":"Loading LoRA adapter...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/peft/config.py:165: UserWarning: Unexpected keyword arguments ['alora_invocation_tokens', 'arrow_config', 'ensure_weight_tying', 'peft_version', 'target_parameters'] for class LoraConfig, these are ignored. This probably means that you're loading a configuration file that was saved using a higher version of the library and additional parameters have been introduced since. It is highly recommended to upgrade the PEFT version before continuing (e.g. by running `pip install -U peft`).\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"âœ… Model + LoRA loaded successfully\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"def generate_response(prompt):\n    # Tokenize\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n\n    # Generate\n    outputs = model.generate(\n        **inputs,\n        max_new_tokens=200,\n        do_sample=False,\n        temperature=0.3,\n        eos_token_id=tokenizer.eos_token_id,\n        pad_token_id=tokenizer.eos_token_id\n    )\n\n    # Decode\n    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    text = decoded.strip()\n\n    # Fallback Ù„Ùˆ Ø§Ù„Ø±Ø¯ Ù‚ØµÙŠØ± Ø£Ùˆ Ù…Ø´ Ù…ÙÙ‡ÙˆÙ…\n    if len(text.split()) < 5:\n        text = (\n            \"I'm really sorry you're feeling this way. \"\n            \"You're not alone, and it's okay to take things one step at a time.\"\n        )\n\n    return text\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T23:33:57.086426Z","iopub.execute_input":"2025-12-17T23:33:57.086698Z","iopub.status.idle":"2025-12-17T23:33:57.091639Z","shell.execute_reply.started":"2025-12-17T23:33:57.086672Z","shell.execute_reply":"2025-12-17T23:33:57.090974Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"import gradio as gr\n\n# =========================\n# Chat function\n# =========================\ndef chat_fn(message, history):\n    response = generate_response(message)\n    history.append((message, response))\n    return history, \"\"\n\n# =========================\n# Custom CSS (Clear + Girly + Centered Title)\n# =========================\ncustom_css = \"\"\"\nbody {\n    background: linear-gradient(135deg, #1f1f2e, #2b2b45);\n}\n\n.gradio-container {\n    font-family: 'Poppins', sans-serif;\n}\n\n/* Center title */\n#title {\n    text-align: center;\n    color: #ffd6ea;\n}\n\n/* Chat container */\n#chatbot {\n    background-color: #121212;\n    border-radius: 22px;\n    padding: 12px;\n}\n\n/* User bubble */\n.message.user {\n    background: linear-gradient(135deg, #ff6fae, #ff8ccf) !important;\n    color: #1a1a1a;\n    font-weight: 500;\n    border-radius: 18px !important;\n}\n\n/* Bot bubble */\n.message.bot {\n    color: #000000 !important;  /* <-- UPDATED: Added !important to ensure black text */\n    font-weight: 500;\n    border-radius: 18px !important;\n}\n\n/* Textbox */\ntextarea {\n    background-color: #1e1e2e !important;\n    color: #ffffff !important;\n    border-radius: 14px !important;\n    border: 1px solid #ff8ccf !important;\n}\n\n/* Buttons */\nbutton {\n    background: linear-gradient(90deg, #ff6fae, #ff8ccf) !important;\n    color: #ffffff ;\n    border-radius: 14px !important;\n    font-weight: bold;\n}\n\nbutton:hover {\n    opacity: 0.92;\n}\n\"\"\"\n\n# =========================\n# Gradio UI\n# =========================\nwith gr.Blocks(css=custom_css) as demo:\n    \n    gr.Markdown(\n        \"\"\"\n        <h2 id=\"title\">ğŸŒ¸ InnerBloom ğŸŒ¸</h2>\n        <p style=\"text-align:center; color:#fbcfe8;\">\n        ğŸ’– Your safe space to talk & bloom ğŸ’–\n        </p>\n        \"\"\"\n    )\n\n    chatbot = gr.Chatbot(elem_id=\"chatbot\", height=420)\n\n    with gr.Row():\n        msg = gr.Textbox(\n            placeholder=\"Type your message here ğŸ’¬\",\n            show_label=False\n        )\n        send = gr.Button(\"Send ğŸ’Œ\")\n\n    clear = gr.Button(\"Clear Chat ğŸ§¹\")\n\n    send.click(chat_fn, inputs=[msg, chatbot], outputs=[chatbot, msg])\n    msg.submit(chat_fn, inputs=[msg, chatbot], outputs=[chatbot, msg])\n    clear.click(lambda: [], None, chatbot)\n\n# To run this, you would execute the script in your terminal.\ndemo.launch()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T23:33:57.092384Z","iopub.execute_input":"2025-12-17T23:33:57.092637Z","iopub.status.idle":"2025-12-17T23:34:02.124767Z","shell.execute_reply.started":"2025-12-17T23:33:57.092619Z","shell.execute_reply":"2025-12-17T23:34:02.124181Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_47/3522074658.py:86: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n  chatbot = gr.Chatbot(elem_id=\"chatbot\", height=420)\n","output_type":"stream"},{"name":"stdout","text":"* Running on local URL:  http://127.0.0.1:7860\nIt looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n\n* Running on public URL: https://ef1a3dd2bd3a9435cd.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://ef1a3dd2bd3a9435cd.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":""},"metadata":{}}],"execution_count":8}]}